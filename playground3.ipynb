{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import torchaudio\n",
    "from transformers import Wav2Vec2ForSequenceClassification, AutoFeatureExtractor\n",
    "import torch\n",
    "\n",
    "class LID:\n",
    "    def __init__(self, device='cpu'):\n",
    "        model_id = \"facebook/mms-lid-256\"\n",
    "\n",
    "        self.processor = AutoFeatureExtractor.from_pretrained(model_id)\n",
    "        self.model = Wav2Vec2ForSequenceClassification.from_pretrained(model_id)\n",
    "        self.device = torch.device(device)\n",
    "        self.model.to(device)\n",
    "    def infer_lid_distribution_batch(self, waveform_chunks, sample_rate, threshold=1):\n",
    "        \"\"\"\n",
    "        Returns the language probability distribution for multiple waveform chunks.\n",
    "\n",
    "        Args:\n",
    "            waveform_chunks (List[Tensor] or Tensor): List of 1D tensors or 2D tensor (batch_size, sequence_length) of audio samples.\n",
    "            sample_rate (int): Sampling rate of the audio.\n",
    "            threshold (float): top languages that explain certain percentages of the distribution. Default as 1 to return all languages.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict[str, float]]: List of mappings from language code to probability for each chunk.\n",
    "        \"\"\"\n",
    "        # Handle both list of tensors and batched tensor inputs\n",
    "        if isinstance(waveform_chunks, list):\n",
    "            # Process as a batch - pad sequences to same length if needed\n",
    "            inputs = self.processor(waveform_chunks, sampling_rate=sample_rate, return_tensors=\"pt\", padding=True)\n",
    "        else:\n",
    "            # Assume it's already a batched tensor\n",
    "            inputs = self.processor(waveform_chunks, sampling_rate=sample_rate, return_tensors=\"pt\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = self.model(**inputs).logits  # Shape: (batch_size, num_classes)\n",
    "            probs = torch.softmax(logits, dim=-1)  # Shape: (batch_size, num_classes)\n",
    "\n",
    "        batch_results = []\n",
    "\n",
    "        # Process each sample in the batch\n",
    "        for batch_idx in range(probs.shape[0]):\n",
    "            sample_probs = probs[batch_idx]\n",
    "\n",
    "            # Map probabilities to language labels for this sample\n",
    "            lang_distribution = {\n",
    "                self.model.config.id2label[idx]: prob.item()\n",
    "                for idx, prob in enumerate(sample_probs)\n",
    "            }\n",
    "\n",
    "            sorted_langs = sorted(lang_distribution.items(), key=lambda x: x[1], reverse=True)\n",
    "            cumulative_prob = 0.0\n",
    "            selected_langs = {}\n",
    "\n",
    "            for lang, prob in sorted_langs:\n",
    "                selected_langs[lang] = prob\n",
    "                cumulative_prob += prob\n",
    "                if cumulative_prob >= threshold:\n",
    "                    break\n",
    "\n",
    "            batch_results.append(selected_langs)\n",
    "\n",
    "        return batch_results\n"
   ],
   "id": "b9580068cfbc72b6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T21:40:50.646145Z",
     "start_time": "2025-07-28T21:40:50.086772Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Wav2Vec2ForSequenceClassification, AutoFeatureExtractor\n",
    "import torch\n",
    "\n",
    "model_id = \"facebook/mms-lid-256\"\n",
    "\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_id)\n",
    "model.to('cpu')"
   ],
   "id": "87f0c1fb66e730da",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Wav2Vec2ForSequenceClassification(\n",
       "  (wav2vec2): Wav2Vec2Model(\n",
       "    (feature_extractor): Wav2Vec2FeatureEncoder(\n",
       "      (conv_layers): ModuleList(\n",
       "        (0): Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(1, 512, kernel_size=(10,), stride=(5,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (1-4): 4 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(3,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "        (5-6): 2 x Wav2Vec2LayerNormConvLayer(\n",
       "          (conv): Conv1d(512, 512, kernel_size=(2,), stride=(2,))\n",
       "          (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation): GELUActivation()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (feature_projection): Wav2Vec2FeatureProjection(\n",
       "      (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "      (projection): Linear(in_features=512, out_features=1280, bias=True)\n",
       "      (dropout): Dropout(p=0.05, inplace=False)\n",
       "    )\n",
       "    (encoder): Wav2Vec2EncoderStableLayerNorm(\n",
       "      (pos_conv_embed): Wav2Vec2PositionalConvEmbedding(\n",
       "        (conv): ParametrizedConv1d(\n",
       "          1280, 1280, kernel_size=(128,), stride=(1,), padding=(64,), groups=16\n",
       "          (parametrizations): ModuleDict(\n",
       "            (weight): ParametrizationList(\n",
       "              (0): _WeightNorm()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (padding): Wav2Vec2SamePadLayer()\n",
       "        (activation): GELUActivation()\n",
       "      )\n",
       "      (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.05, inplace=False)\n",
       "      (layers): ModuleList(\n",
       "        (0-47): 48 x Wav2Vec2EncoderLayerStableLayerNorm(\n",
       "          (attention): Wav2Vec2SdpaAttention(\n",
       "            (k_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (v_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (q_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "            (out_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          )\n",
       "          (dropout): Dropout(p=0.05, inplace=False)\n",
       "          (layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (feed_forward): Wav2Vec2FeedForward(\n",
       "            (intermediate_dropout): Dropout(p=0.05, inplace=False)\n",
       "            (intermediate_dense): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "            (output_dense): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "            (output_dropout): Dropout(p=0.05, inplace=False)\n",
       "          )\n",
       "          (final_layer_norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "          (adapter_layer): Wav2Vec2AttnAdapterLayer(\n",
       "            (norm): LayerNorm((1280,), eps=1e-05, elementwise_affine=True)\n",
       "            (linear_1): Linear(in_features=1280, out_features=16, bias=True)\n",
       "            (act_fn): ReLU()\n",
       "            (linear_2): Linear(in_features=16, out_features=1280, bias=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (projector): Linear(in_features=1280, out_features=1024, bias=True)\n",
       "  (classifier): Linear(in_features=1024, out_features=256, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-28T21:15:53.336386Z",
     "start_time": "2025-07-28T21:15:51.730142Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import time\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from transformers import Wav2Vec2ForSequenceClassification, AutoFeatureExtractor\n",
    "\n",
    "model_id = \"facebook/mms-lid-256\"\n",
    "processor = AutoFeatureExtractor.from_pretrained(model_id)\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "def create_dummy_batch_tensor(batch_size: int, duration_seconds: float = 5.0, sample_rate: int = 16000):\n",
    "    \"\"\"\n",
    "    Create a batch of dummy audio as a single batched tensor.\n",
    "\n",
    "    Returns:\n",
    "        2d matrix in list[list[float]]\n",
    "    \"\"\"\n",
    "    num_samples = int(duration_seconds * sample_rate)\n",
    "    return (torch.randn(batch_size, num_samples) * 0.1).tolist()\n",
    "\n",
    "\n",
    "# Batch function\n",
    "def infer_lid_distribution_batch(waveform_chunks, sample_rate, threshold=1):\n",
    "    \"\"\"Batch version\"\"\"\n",
    "    if isinstance(waveform_chunks, list):\n",
    "        inputs = processor(waveform_chunks, sampling_rate=sample_rate, return_tensors=\"pt\", padding=True)\n",
    "    else:\n",
    "        inputs = processor(waveform_chunks, sampling_rate=sample_rate, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        probs = torch.softmax(logits, dim=-1)\n",
    "\n",
    "\n",
    "def benchmark_single_vs_batch(batch_sizes: List[int], num_runs: int = 3):\n",
    "    \"\"\"\n",
    "    Benchmark single vs batch processing across different batch sizes.\n",
    "\n",
    "    Args:\n",
    "        batch_sizes: List of batch sizes to test\n",
    "        num_runs: Number of runs to average over\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    print(f\"Benchmarking Language ID: Single vs Batch Processing\")\n",
    "    print(f\"Audio: 30s chunks at 16kHz ({30*16000:,} samples per chunk)\")\n",
    "    print(f\"Runs per test: {num_runs}\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        print(f\"\\nTesting batch size: {batch_size}\")\n",
    "\n",
    "        # Create test data\n",
    "\n",
    "        waveform_tensor = create_dummy_batch_tensor(batch_size)\n",
    "\n",
    "\n",
    "        # Time batch processing\n",
    "        batch_times = []\n",
    "        for run in range(num_runs):\n",
    "            start_time = time.time()\n",
    "            infer_lid_distribution_batch(waveform_tensor, 16000)\n",
    "            end_time = time.time()\n",
    "            batch_times.append(end_time - start_time)\n",
    "\n",
    "        avg_batch_time = np.mean(batch_times)\n",
    "\n",
    "        print(f\"Time: {avg_batch_time:.3f}s \")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Full benchmark across different batch sizes\n",
    "    batch_sizes_to_test = [1]\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"RUNNING FULL BENCHMARK\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    benchmark_single_vs_batch(\n",
    "        batch_sizes=batch_sizes_to_test,\n",
    "        num_runs=1\n",
    "    )\n",
    "\n"
   ],
   "id": "cecc9a3ff0667734",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "RUNNING FULL BENCHMARK\n",
      "============================================================\n",
      "Benchmarking Language ID: Single vs Batch Processing\n",
      "Audio: 30s chunks at 16kHz (480,000 samples per chunk)\n",
      "Runs per test: 1\n",
      "------------------------------------------------------------\n",
      "\n",
      "Testing batch size: 1\n",
      "Time: 1.022s \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Quick test with batch size 2:\n",
    "list\n",
    "Created 2 waveforms, each with 480,000 samples\n",
    "Batch processing took: 13.467s\n",
    "Got 2 results\n",
    "Sample result keys: ['ara', 'cmn', 'eng', 'spa', 'fra']...\n",
    "\n",
    "============================================================\n",
    "RUNNING FULL BENCHMARK\n",
    "============================================================\n",
    "Benchmarking Language ID: Single vs Batch Processing\n",
    "Audio: 30s chunks at 16kHz (480,000 samples per chunk)\n",
    "Threshold: 0.95\n",
    "Runs per test: 3\n",
    "------------------------------------------------------------\n",
    "\n",
    "Testing batch size: 1\n",
    "  Single processing: 2.529s (±0.006s)\n",
    "  Batch processing:  2.697s (±0.088s)\n",
    "  Speedup: 0.94x\n",
    "\n",
    "Testing batch size: 2\n",
    "  Single processing: 5.413s (±0.130s)\n",
    "  Batch processing:  4.944s (±0.011s)\n",
    "  Speedup: 1.10x\n",
    "\n",
    "Testing batch size: 4\n",
    "  Single processing: 10.646s (±0.146s)\n",
    "  Batch processing:  9.600s (±0.058s)\n",
    "  Speedup: 1.11x\n",
    "\n",
    "Testing batch size: 8\n",
    "  Single processing: 21.354s (±0.094s)\n",
    "  Batch processing:  18.376s (±0.064s)\n",
    "  Speedup: 1.16x\n",
    "\n",
    "Testing batch size: 16\n",
    "  Single processing: 45.138s (±0.306s)\n",
    "  Batch processing:  42.045s (±0.465s)\n",
    "  Speedup: 1.07x\n"
   ],
   "id": "8dc05ff195efbd8b"
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
