{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:49:23.158162Z",
     "start_time": "2025-07-19T22:49:23.015670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "import librosa\n",
    "import torchaudio\n",
    "import soundfile as sf\n",
    "import pandas as pd\n",
    "# from src.data.preproc import convert_libri\n",
    "from src.data.util import log_compress, read_audioset_csv\n",
    "from src.data.vad import trim_speech"
   ],
   "id": "ae60e99e85bfcc9f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# Take from processed first, then filter for whatever setting and then sample from that.",
   "id": "86e10da8edfd1c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T16:41:59.453440Z",
     "start_time": "2025-07-26T16:41:53.160315Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.data.lid import LID\n",
    "import torch\n",
    "lid = LID(torch.device('cuda'))\n",
    "for param in lid.model.parameters():\n",
    "    print(f'Model weight type: {param.dtype}')\n",
    "    break"
   ],
   "id": "85a11e34aaced957",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/larry/miniconda3/envs/speech-health-ai/lib/python3.12/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n",
      "/home/larry/miniconda3/envs/speech-health-ai/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weight type: torch.float32\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-26T16:43:07.472441Z",
     "start_time": "2025-07-26T16:43:05.915992Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.data.vad import VAD\n",
    "vad = VAD(torch.device('cuda'))\n",
    "for param in vad.model.parameters():\n",
    "    print(f'Model weight type: {param.dtype}')\n",
    "    break"
   ],
   "id": "c29ead0b21bb702f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weight type: torch.float32\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-20T18:30:26.703917Z",
     "start_time": "2025-07-20T18:30:26.699487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Sample an audio from paths to listen\n",
    "import random\n",
    "root_path = '/Users/lkieu/Desktop/Audioset/processed/'\n",
    "ref = '/Users/lkieu/Desktop/Audioset/audio/bal_train'\n",
    "\n",
    "paths = glob.glob(os.path.join(root_path, '**', '*.flac'), recursive=True)\n",
    "sampled_file = random.choice(paths)\n",
    "ref = os.path.join(ref, sampled_file.split('/')[-1])\n",
    "print(\"Sampled:\", sampled_file)\n",
    "\n"
   ],
   "id": "180190d16a24334d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled: /Users/lkieu/Desktop/Audioset/processed/7bvl9662aVc.flac\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-21T20:27:30.986485Z",
     "start_time": "2025-07-21T20:27:30.777086Z"
    }
   },
   "cell_type": "code",
   "source": [
    "play_audio(sampled_file)\n",
    "play_audio(ref)"
   ],
   "id": "a133335ff68c6fd5",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'play_audio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mplay_audio\u001B[49m(sampled_file)\n\u001B[1;32m      2\u001B[0m play_audio(ref)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'play_audio' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:43:17.855100Z",
     "start_time": "2025-07-19T22:43:17.844695Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.data.util import resample\n",
    "\n",
    "test_flac = 'testVADandLID.m4a'\n",
    "\n",
    "waveform, sr = torchaudio.load(test_flac, normalize=True)\n",
    "resample(waveform, sr)\n"
   ],
   "id": "bbdf2be2fbb248b7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:43:08.123623Z",
     "start_time": "2025-07-19T22:43:08.121848Z"
    }
   },
   "cell_type": "code",
   "source": "waveform = waveform.squeeze()",
   "id": "ed90a5bbc79e8b5c",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:43:19.209287Z",
     "start_time": "2025-07-19T22:43:19.021220Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from pyannote.audio.pipelines import VoiceActivityDetection\n",
    "# instantiate the model\n",
    "from pyannote.audio import Model\n",
    "model = Model.from_pretrained(\n",
    "  \"pyannote/segmentation-3.0\")\n",
    "pipeline = VoiceActivityDetection(segmentation=model)\n",
    "HYPER_PARAMETERS = {\n",
    "  # remove speech regions shorter than that many seconds.\n",
    "  \"min_duration_on\": 0.0,\n",
    "  # fill non-speech regions shorter than that many seconds.\n",
    "  \"min_duration_off\": 0.0\n",
    "}\n",
    "pipeline.instantiate(HYPER_PARAMETERS)\n",
    "vad = pipeline({'waveform': waveform, 'sample_rate': sr})\n",
    "# `vad` is a pyannote.core.Annotation instance containing speech regions\n",
    "str(vad)"
   ],
   "id": "ba117b31733a6759",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ 00:00:00.030 -->  00:00:02.325] 0 SPEECH\\n[ 00:00:02.882 -->  00:00:04.739] 0 SPEECH'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:49:24.355126Z",
     "start_time": "2025-07-19T22:49:24.352688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "vi_segment = '[ 00:00:00.030 -->  00:00:02.325] 0 SPEECH'\n",
    "en_segment = '[ 00:00:02.882 -->  00:00:04.739] 0 SPEECH'\n",
    "vi_waveform = trim_speech(waveform, 16000, vi_segment)\n",
    "en_waveform = trim_speech(waveform, 16000, en_segment)"
   ],
   "id": "d47a79060598d82a",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-19T22:50:27.229578Z",
     "start_time": "2025-07-19T22:50:25.988033Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import Wav2Vec2ForSequenceClassification, AutoFeatureExtractor\n",
    "import torch\n",
    "\n",
    "model_id = \"facebook/mms-lid-256\"\n",
    "\n",
    "processor = AutoFeatureExtractor.from_pretrained(model_id)\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_id)\n",
    "waveform = en_waveform\n",
    "# English\n",
    "inputs = processor(waveform.squeeze(), sampling_rate=16_000, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs).logits\n",
    "\n",
    "prob = torch.argmax(outputs, dim=-1)\n",
    "lang_id = prob[0].item()\n",
    "detected_lang = model.config.id2label[lang_id]\n",
    "print(detected_lang)\n",
    "# 'eng'\n"
   ],
   "id": "a235d071ef4d61f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jav\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T18:01:36.513701Z",
     "start_time": "2025-07-17T18:01:36.511275Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lat'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 22,
   "source": "detected_lang",
   "id": "ca93d2d2e586b0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T20:14:50.002169Z",
     "start_time": "2025-07-17T20:14:49.977480Z"
    }
   },
   "cell_type": "code",
   "source": [
    "path = '/Users/lkieu/PycharmProjects/PhonemeAwareFoundational/test_audio/balanced_train_segments.csv'\n",
    "df = read_audioset_csv(path)\n",
    "df.head(3)"
   ],
   "id": "77dd223c86314309",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "          YTID  start_seconds  end_seconds                 positive_labels\n",
       "0  --PJHxphWEs           30.0         40.0          [/m/09x0r, /t/dd00088]\n",
       "1  --ZhevVpy1s           50.0         60.0                     [/m/012xff]\n",
       "2  --aE2O5G5WE            0.0         10.0  [/m/03fwl, /m/04rlf, /m/09x0r]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YTID</th>\n",
       "      <th>start_seconds</th>\n",
       "      <th>end_seconds</th>\n",
       "      <th>positive_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--PJHxphWEs</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>[/m/09x0r, /t/dd00088]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>--ZhevVpy1s</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>[/m/012xff]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--aE2O5G5WE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[/m/03fwl, /m/04rlf, /m/09x0r]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T20:14:38.297239Z",
     "start_time": "2025-07-17T20:14:38.262900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from src.data.proc_audio_set import convert_audio_set, has_allowed_tag\n",
    "\n",
    "audio_path = '/Users/lkieu/Desktop/Audioset/audio/bal_train'\n",
    "paths = convert_audio_set(audio_path)\n",
    "\n",
    "# Filter for speech tags\n",
    "df = read_audioset_csv(path)\n",
    "ytids_to_paths = {\n",
    "        os.path.splitext(os.path.basename(p))[0]: p for p in paths\n",
    "    }\n",
    "\n",
    "ytids = list(ytids_to_paths.keys())\n",
    "df_filtered = df[df['YTID'].isin(ytids)].copy()\n",
    "df_filtered['has_allowed_tag'] = df_filtered['positive_labels'].apply(has_allowed_tag)\n",
    "df_result = df_filtered[df_filtered['has_allowed_tag']==True ].copy()\n",
    "len(df_result)"
   ],
   "id": "68e9ae1685d0d23b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "534"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-07-17T20:34:26.887802Z",
     "start_time": "2025-07-17T20:34:26.883181Z"
    }
   },
   "cell_type": "code",
   "source": "df_result.head(3)",
   "id": "f75f81c46f71c756",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "           YTID  start_seconds  end_seconds                  positive_labels  \\\n",
       "0   --PJHxphWEs           30.0         40.0           [/m/09x0r, /t/dd00088]   \n",
       "2   --aE2O5G5WE            0.0         10.0   [/m/03fwl, /m/04rlf, /m/09x0r]   \n",
       "25  -30H9V1IKps            6.0         16.0  [/m/07yv9, /m/09x0r, /m/0gvgw0]   \n",
       "\n",
       "    has_allowed_tag  \n",
       "0              True  \n",
       "2              True  \n",
       "25             True  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>YTID</th>\n",
       "      <th>start_seconds</th>\n",
       "      <th>end_seconds</th>\n",
       "      <th>positive_labels</th>\n",
       "      <th>has_allowed_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>--PJHxphWEs</td>\n",
       "      <td>30.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>[/m/09x0r, /t/dd00088]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>--aE2O5G5WE</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>[/m/03fwl, /m/04rlf, /m/09x0r]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-30H9V1IKps</td>\n",
       "      <td>6.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>[/m/07yv9, /m/09x0r, /m/0gvgw0]</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Gather all the flac and json\n",
    "data_dir = \"./LibriLight/small\"\n",
    "\n",
    "def gather_flac_json_pairs(root_dir):\n",
    "    flac_files = glob.glob(os.path.join(root_dir, '**', '*.flac'), recursive=True)\n",
    "    pairs = []\n",
    "\n",
    "    for flac_path in flac_files:\n",
    "        json_path = os.path.splitext(flac_path)[0] + '.json'\n",
    "        if os.path.exists(json_path):\n",
    "            pairs.append((flac_path, json_path))\n",
    "        else:\n",
    "            print(f\"Warning: No JSON companion for {flac_path}\")\n",
    "\n",
    "    return pairs\n",
    "\n",
    "pairs = gather_flac_json_pairs(data_dir)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "def find_duplicate_filenames(paths):\n",
    "    filename_to_paths = defaultdict(list)\n",
    "\n",
    "    for path in paths:\n",
    "        filename = os.path.basename(path)\n",
    "        filename_to_paths[filename].append(path)\n",
    "\n",
    "    duplicates = {fname: plist for fname, plist in filename_to_paths.items() if len(plist) > 1}\n",
    "\n",
    "    for fname, plist in duplicates.items():\n",
    "        print(f\"Duplicate filename: {fname}\")\n",
    "        for p in plist:\n",
    "            print(f\"  {p}\")\n",
    "\n",
    "paths = list(map(lambda x: x[0], pairs))\n",
    "find_duplicate_filenames(paths)\n"
   ],
   "id": "94ee1b3aaf636bfa",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Verify Sampling rate\n",
    "def get_sample_rate(path):\n",
    "    metadata = sf.info(path)\n",
    "    return metadata.samplerate\n",
    "\n",
    "sample_rate = {}\n",
    "for path, _ in pairs:\n",
    "    info = f'{get_sample_rate(path)} hz'\n",
    "    if info not in sample_rate:\n",
    "        sample_rate[info] = 1\n",
    "    else:\n",
    "        sample_rate[info] += 1\n",
    "print(sample_rate)"
   ],
   "id": "714036b28fa0d6cd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Distribution of Voice Activity block length.\n",
    "# Specific to LibriLight\n",
    "from collections import Counter\n",
    "import json\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def collect_duration_differences(flac_json_pairs):\n",
    "    all_durations = []\n",
    "\n",
    "    for flac_path, json_path in flac_json_pairs:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        voice_activity = data.get('voice_activity', [])\n",
    "        durations = [end - start for start, end in voice_activity]\n",
    "        all_durations.extend(durations)\n",
    "\n",
    "    return all_durations\n",
    "\n",
    "def plot_duration_distribution(durations, bins=50):\n",
    "    plt.hist(durations, bins=bins, edgecolor='black')\n",
    "    plt.title('Distribution of Voice Activity Durations')\n",
    "    plt.xlabel('Duration (seconds)')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n",
    "def top_n_durations(durations, n=10, rounding=2):\n",
    "    rounded_durations = [round(d, rounding) for d in durations]\n",
    "    counter = Counter(rounded_durations)\n",
    "    most_common = counter.most_common(n)\n",
    "    return most_common\n",
    "\n",
    "\n",
    "durations = collect_duration_differences(pairs)\n",
    "print(top_n_durations(durations))\n",
    "print('min: ' + str(min(durations)))\n",
    "print('max: ' + str(max(durations)))\n",
    "plot_duration_distribution(durations)"
   ],
   "id": "19910aa415b7ef85",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# What if we allow for silence of 1s\n",
    "\n",
    "def merge_close_blocks(voice_activity, threshold=1.0):\n",
    "    if not voice_activity:\n",
    "        return []\n",
    "\n",
    "    # Sort by start time just in case\n",
    "    voice_activity = sorted(voice_activity, key=lambda x: x[0])\n",
    "    merged = [voice_activity[0]]\n",
    "\n",
    "    for start, end in voice_activity[1:]:\n",
    "        last_start, last_end = merged[-1]\n",
    "        if start - last_end < threshold:\n",
    "            # Merge intervals\n",
    "            merged[-1][1] = max(last_end, end)\n",
    "        else:\n",
    "            merged.append([start, end])\n",
    "\n",
    "    return merged\n",
    "\n",
    "def get_duration_diff_merged(pairs):\n",
    "    all_durations = []\n",
    "\n",
    "    for flac_path, json_path in pairs:\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        voice_activity = data.get('voice_activity', [])\n",
    "        voice_activity = merge_close_blocks(voice_activity)\n",
    "        durations = [end - start for start, end in voice_activity]\n",
    "        all_durations.extend(durations)\n",
    "\n",
    "    return all_durations\n",
    "\n",
    "durations = get_duration_diff_merged(pairs)\n",
    "print(top_n_durations(durations))\n",
    "print('min: ' + str(min(durations)))\n",
    "print('max: ' + str(max(durations)))\n",
    "plot_duration_distribution(durations)"
   ],
   "id": "92d9b7523980b4f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "a = torch.tensor([[1,2],\n",
    "                  [3,4]])\n",
    "b = torch.tensor([[5,6],\n",
    "                 [7,8]])\n",
    "torch.maximum(a, a.max() - 1)"
   ],
   "id": "39956b310a7a9918",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchaudio import transforms\n",
    "import torchaudio\n",
    "\n",
    "test_flac = '/Users/lkieu/PycharmProjects/PhonemeAwareFoundational/test_audio/canterburytales_09_chaucer_64kb.flac'\n",
    "waveform, sample_rate = torchaudio.load(test_flac, normalize=True)\n",
    "transform = transforms.MelSpectrogram(sample_rate=sample_rate, n_fft=1024, hop_length=256, n_mels=80)\n",
    "melspec = transform(waveform)\n",
    "# Define max size (in time frames)\n",
    "max_frames = 500  # Example\n",
    "\n",
    "def crop_or_pad(spec, max_frames):\n",
    "    channels, n_mels, time_frames = spec.shape\n",
    "    if time_frames > max_frames:\n",
    "        return spec[:, :, :max_frames]\n",
    "    elif time_frames < max_frames:\n",
    "        pad_amount = max_frames - time_frames\n",
    "        pad = torch.zeros((channels, n_mels, pad_amount), device=spec.device)\n",
    "        return torch.cat((spec, pad), dim=2)\n",
    "    else:\n",
    "        return spec\n",
    "\n",
    "melspec= crop_or_pad(melspec, max_frames)\n",
    "print(melspec.shape)  # Should be (channels, n_mels, max_frames)\n"
   ],
   "id": "f0cb4bcf1b148a21",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.data.util import log_compress\n",
    "import librosa\n",
    "\n",
    "log_spec_lib = librosa.power_to_db(melspec[0])\n",
    "log_spec_torch = log_compress(melspec[0])"
   ],
   "id": "d312169be689a2e0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_spectrogram(specgram, title=None, ylabel=\"freq_bin\", ax=None):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots(1, 1)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.imshow(specgram, origin=\"lower\", aspect=\"auto\", interpolation=\"nearest\")"
   ],
   "id": "379c5a1292d81047",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_spectrogram(log_spec_lib)",
   "id": "1a31040cfa40540e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plot_spectrogram(log_spec_torch)",
   "id": "a758f58022bfc890",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from src.data.util import whisper_norm, z_score_norm, min_max_norm\n",
    "whisper_norm_log_spec = whisper_norm(log_spec_torch)\n",
    "plot_spectrogram(whisper_norm_log_spec)"
   ],
   "id": "961e678b51634d2b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "z_norm = z_score_norm(log_spec_torch)\n",
    "plot_spectrogram(z_norm)"
   ],
   "id": "5edd68455c7250d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "min_max_norm_log_spec = min_max_norm(log_spec_torch)\n",
    "plot_spectrogram(min_max_norm_log_spec)"
   ],
   "id": "7d0b44ecf567244e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "e9ee8950cd0ddd2b",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
